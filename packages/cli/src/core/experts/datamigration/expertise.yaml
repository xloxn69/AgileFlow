# Data Migration Expert - Domain Knowledge
# This file is the agent's "mental model" of the data migration domain
# AUTO-UPDATED by self-improve.md after completing work

domain: datamigration
last_updated: 2025-12-21
version: 1.1

files:
  migrations:
    - path: src/migrations/
      purpose: "Database migration scripts"
      conventions: "Timestamped: YYYYMMDDHHMMSS_description.ts"
    - path: src/migrations/data/
      purpose: "Data transformation scripts"
      conventions: "Separate from schema migrations"
    - path: src/migrations/rollback/
      purpose: "Rollback scripts for each migration"

  validation:
    - path: src/migrations/validators/
      purpose: "Data validation scripts"
      conventions: "Pre and post-migration validators"
    - path: docs/migrations/validation-reports/
      purpose: "Validation report outputs"

  documentation:
    - path: docs/migrations/
      purpose: "Migration documentation"
    - path: docs/migrations/runbooks/
      purpose: "Migration runbooks"
    - path: docs/migrations/history.md
      purpose: "Migration history log"

relationships:
  - parent: schema_migration
    child: data_migration
    type: precedes
    notes: "Schema changes before data transformations"
  - parent: migration
    child: rollback
    type: paired_with
    notes: "Every migration has a rollback script"
  - parent: validation
    child: migration
    type: verifies
    notes: "Validation before and after migration"

patterns:
  - name: Zero-Downtime Migration
    description: "Migrate without taking service offline"
    location: docs/migrations/
    example: |
      1. Add new column (nullable)
      2. Backfill data
      3. Switch application to use new column
      4. Remove old column

  - name: Expand-Contract Pattern
    description: "Add new, migrate data, remove old"
    location: src/migrations/
    example: "Add column → copy data → remove old column"

  - name: Blue-Green Migration
    description: "Run old and new in parallel, switch over"
    location: docs/migrations/
    example: "Write to both, read from old, switch reads, stop old writes"

  - name: Validation Checkpoints
    description: "Validate data at each migration step"
    location: src/migrations/validators/
    example: "Count check, integrity check, sample verification"

conventions:
  - "ALWAYS backup before migration"
  - "ALWAYS have rollback script ready"
  - "NEVER run migrations during peak hours"
  - "Validate data before and after migration"
  - "Test migrations in staging first"
  - "Document migration in runbook"
  - "Monitor during and after migration"
  - "Keep migrations idempotent when possible"

migration_checklist:
  pre_migration:
    - "Backup completed and verified"
    - "Rollback script tested"
    - "Staging migration successful"
    - "Downtime window scheduled (if needed)"
    - "Stakeholders notified"
    - "Monitoring dashboards ready"

  during_migration:
    - "Progress logging enabled"
    - "Error handling active"
    - "Checkpoint validation passing"
    - "Performance within bounds"

  post_migration:
    - "Data validation passed"
    - "Application functioning correctly"
    - "No data loss verified"
    - "Rollback script archived"
    - "Migration documented in history"

# Learnings are AUTO-UPDATED by self-improve.md
learnings:
  - date: 2025-12-21
    context: AgileFlow auto-archival system
    learning: |
      JSON-based data migration pattern: AgileFlow uses scripts/archive-completed-stories.sh
      to automatically migrate completed stories from status.json to archive/YYYY-MM.json files.
      This is a time-based migration (7-day threshold) that keeps the active status file lean
      while preserving historical data in monthly archives. The migration is idempotent and
      runs automatically on session start via SessionStart hook.

  - date: 2025-12-21
    context: AgileFlow status.json compression
    learning: |
      Data compression migration: The scripts/compress-status.sh script performs field-level
      migration by removing verbose fields (like descriptions, acceptance criteria) while
      preserving essential data. This is a vertical migration (reducing fields per record)
      rather than horizontal (moving records between files). Useful for reducing file size
      without losing critical tracking data.

  - date: 2025-12-21
    context: AgileFlow archival configuration
    learning: |
      Configuration-driven migration: Migration behavior is controlled via
      docs/00-meta/agileflow-metadata.json with archival.threshold_days and archival.enabled
      settings. This allows users to customize migration behavior without code changes.
      The threshold_days acts as a temporal boundary for data migration decisions.

  - date: 2025-12-21
    context: AgileFlow JSON data patterns
    learning: |
      JSON file migration differs from SQL migrations: No schema enforcement, no transactions,
      no referential integrity. Instead, rely on: (1) atomic file writes, (2) backup before
      modify, (3) validation via JSON.parse, (4) idempotent scripts that check for existing
      data before migrating. AgileFlow's archival system demonstrates these patterns well -
      it checks if story already exists in archive before moving it.
