# Monitoring Expert - Domain Knowledge
# This file is the agent's "mental model" of the monitoring domain
# AUTO-UPDATED by self-improve.md after completing work

domain: monitoring
last_updated: 2025-12-21
version: 1.1

files:
  logging:
    - path: src/logging/
      purpose: "Structured logging implementation"
      key_exports: [logger, createLogger]
      conventions: "JSON structured logs"
    - path: src/logging/logger.ts
      purpose: "Logger factory and configuration"
      conventions: "Singleton logger instance"
    - path: src/logging/context.ts
      purpose: "Request context for correlation IDs"
      conventions: "AsyncLocalStorage for request context"

  metrics:
    - path: src/metrics/
      purpose: "Application metrics collection"
      key_exports: [metrics, histogram, counter, gauge]
      conventions: "Prometheus-compatible metrics"
    - path: src/metrics/registry.ts
      purpose: "Metrics registry and exporters"
    - path: src/metrics/middleware.ts
      purpose: "HTTP metrics middleware"

  health:
    - path: src/health/
      purpose: "Health check implementations"
      key_exports: [healthCheck, readinessCheck]
      conventions: "Return 200 if healthy, 503 if degraded"
    - path: src/health/checks/
      purpose: "Individual dependency health checks"
      conventions: "One check per dependency"

  alerting:
    - path: monitoring/alerts/
      purpose: "Alert rule definitions"
      conventions: "YAML or Prometheus alerting rules"
    - path: monitoring/runbooks/
      purpose: "Incident response runbooks"
      conventions: "Markdown with diagnosis and resolution steps"

  dashboards:
    - path: monitoring/dashboards/
      purpose: "Grafana dashboard JSON definitions"
      conventions: "One dashboard per service or domain"

relationships:
  - parent: api_endpoints
    child: metrics_middleware
    type: measured_by
    notes: "All API requests measured by metrics middleware"
  - parent: logger
    child: log_aggregator
    type: ships_to
    notes: "Logs shipped to ELK/Datadog/CloudWatch"
  - parent: metrics
    child: prometheus
    type: scraped_by
    notes: "Prometheus scrapes /metrics endpoint"
  - parent: alerts
    child: runbooks
    type: links_to
    notes: "Each alert links to its runbook"

patterns:
  - name: Structured Logging
    description: "JSON logs with correlation IDs and context"
    location: src/logging/
    example: |
      logger.info('Request completed', {
        request_id: ctx.requestId,
        user_id: ctx.userId,
        duration_ms: 142,
        status: 200
      });

  - name: RED Metrics
    description: "Rate, Errors, Duration for every service"
    location: src/metrics/
    example: |
      - http_requests_total (counter)
      - http_request_duration_seconds (histogram)
      - http_request_errors_total (counter)

  - name: Health Check Pattern
    description: "Check all dependencies and return aggregate health"
    location: src/health/
    example: |
      /health returns { status: 'healthy', checks: { db: true, cache: true } }

  - name: Alert with Runbook
    description: "Every alert links to resolution runbook"
    location: monitoring/alerts/
    example: |
      - alert: HighErrorRate
        annotations:
          runbook: /runbooks/high-error-rate.md

  - name: SLO-Based Alerting
    description: "Alert on error budget burn rate, not just thresholds"
    location: monitoring/alerts/
    example: "Alert if burning >2% error budget per hour"

conventions:
  - "NEVER log PII, passwords, or API keys"
  - "ALWAYS include request_id/trace_id in logs"
  - "Use structured JSON logging (not plain text)"
  - "Expose /metrics endpoint for Prometheus"
  - "Expose /health endpoint for load balancer"
  - "Link every alert to a runbook"
  - "Use log levels appropriately: ERROR, WARN, INFO, DEBUG"
  - "Set retention policies for logs and metrics"
  - "Test alerts in staging before production"

log_levels:
  ERROR: "Service unavailable, data loss, requires immediate attention"
  WARN: "Degraded behavior, unexpected but handled, investigate soon"
  INFO: "Important state changes, deployments, audit events"
  DEBUG: "Detailed diagnostic info, development only"

metric_types:
  counter: "Cumulative value (requests_total, errors_total)"
  gauge: "Current value (active_connections, queue_size)"
  histogram: "Distribution (request_duration, response_size)"
  summary: "Quantiles (p50, p90, p99 latency)"

slo_targets:
  availability: "99.9% (8.7 hours downtime/year)"
  latency_p50: "<100ms"
  latency_p95: "<200ms"
  latency_p99: "<500ms"
  error_rate: "<0.1%"

# Learnings are AUTO-UPDATED by self-improve.md
# Do not edit manually - let the agent learn from experience
learnings:
  - date: 2025-12-21
    context: "AgileFlow monitoring patterns"
    learning: |
      Session hooks run on SessionStart via .claude/settings.json hooks configuration.
      The default SessionStart hook executes 'node .agileflow/scripts/get-env.js' to display
      project info, git status, and system details at session start. This provides
      immediate context about the development environment.

  - date: 2025-12-21
    context: "AgileFlow status line implementation"
    learning: |
      Status line shows real-time project context in Claude Code status bar via
      scripts/agileflow-statusline.sh. Components include: git branch (color-coded),
      current story from status.json, WIP count (yellow if >1, red if >3), and
      completion percentage. This provides at-a-glance project health visibility.

  - date: 2025-12-21
    context: "AgileFlow agent activity logging"
    learning: |
      Agent communication is logged to docs/09-agents/bus/log.jsonl in append-only
      format. Each entry is a JSON line with timestamp, agent_id, message_type, and
      payload. This enables audit trails and debugging of multi-agent workflows
      without corrupting the log file with concurrent writes.

  - date: 2025-12-21
    context: "AgileFlow validation script patterns"
    learning: |
      Validation scripts (e.g., scripts/validate-*.sh) output PASS/WARN/FAIL status
      codes with colored output for health checks. Scripts validate configuration,
      file structure, and dependencies. Exit code 0 = all checks passed, non-zero =
      failures detected. This standardized format enables automated CI/CD integration.
